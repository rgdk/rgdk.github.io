---
title: "Creating a Prediction Model for Exercise Style Using Accelerometric Data"
date: "Thursday, February 19, 2015"
output: html_document
---

##Introduction
The following analysis is based on data from the 2013 Human Activity Recognition study by Wallace, Velloso and Fuks, **"Qualitative Activity Recognition of Weight Lifting Exercises"**. In it, the researchers record accelometric data from six subjects performing repetitions of the Unilateral Dumbbell Biceps Curl exercise. Each set of repetitions was done in five different ways. These are (as outlined in their paper):

* Exactly according to the specification (Class A) 
* Throwing the elbows to the front (Class B)
* Lifting the dumbbell only halfway (Class C) 
* Lowering the dumbbell only halfway (Class D) 
* Throwing the hips to the front (Class E)

Read more at: http://groupware.les.inf.puc-rio.br/har#ixzz3SNZVfYmr

The present paper utlises this data to identify a suitable model to be used to predict the manner in which the exercise was carried out (classes A-E), which is stored in the variable 'classe'.

##Data Preparation

First install the required libraries that will be referenced during the analysis.
```{r}
suppressWarnings(library(caret))
suppressWarnings(library(e1071))
suppressWarnings(library(corrplot))
suppressWarnings(library(survival))
```

Next retrieve the data sets - note that the non-readable strings found in the data set are set to Null by the read.csv() function.
```{r}
setwd("C:/Training/DataScienceJHU/PracticalMachineLearning/Project")
testing <- read.csv("pml-testing.csv", stringsAsFactors=TRUE, na.strings=c("NA","","#DIV/0!"))
training <- read.csv("pml-training.csv", stringsAsFactors=TRUE, na.strings=c("NA","","#DIV/0!"))
```

Display the numbers of samples and variables for both the training and test data sets
```{r}
dim(training)
dim(testing)
```

Remove the null-only columns from the data set
```{r}
non_null_cols <- names(training)[apply(X=training, MARGIN=2, FUN=function(x) !sum(is.na(x[1])))]
trainingSubset <- training[,non_null_cols]
dim(trainingSubset)
```

100 variables have now been discarded, leaving the following 60 contenders.
```{r}
names(trainingSubset)
```

We notice that some of the remaining variables are non-measurement data columns. These are now removed.
```{r}
measure_cols <- names(trainingSubset)[!(names(trainingSubset) %in% 
                                          c("X",
                                            "user_name",
                                            "raw_timestamp_part_1",
                                            "raw_timestamp_part_2",
                                            "cvtd_timestamp",
                                            "new_window",
                                            "num_window"))]

trainingSubset <- trainingSubset[,names(trainingSubset) %in% measure_cols]
```

Determine if there are near zero covariates amonst the remaining variables.
```{r}
nzv <- nearZeroVar(trainingSubset, saveMetrics=TRUE)
nzv
```

It appears that there are none to be removed. 

Determine which variables remain 
```{r}
names(trainingSubset)
```

Look at the distribution of values in the classe variable that is to be predicted. 
```{r}
table(trainingSubset$classe)
```

It appears that there are more in class A than any other class.

Determine any near-perfectly correlated variables which we define as those with a greater correlation than +/-0.8
```{r}
correlValues <- abs(cor(trainingSubset[,-53]))
diag(correlValues) <- 0
```

Plot the correlations
```{r}
corrplot(correlValues, method="square", type = "lower", tl.cex=0.8)
```

Determine the column indices of the variables with high correlation and display them
```{r}
highCorr <- findCorrelation(correlValues[,-53], cutoff = .80)     # high correlation
names(trainingSubset[,highCorr])
```

Remove these highly correlated variables and display the remaining variables
```{r}
trainingSubset<-trainingSubset[,-highCorr]    
length(names(trainingSubset))
```

Now divide and train the training data.
```{r}
set.seed(56789)
inTrain = createDataPartition(y=trainingSubset$classe, p=0.75, list=FALSE)
traindata = trainingSubset[ inTrain,]
testdata =  trainingSubset[-inTrain,]
```

##Setup the prediction model

We will evaluate three different models before settling on a final choice.

First we have elected to use a Linear Discriminant Analysis (LDA) model with cross-validation involving 3 times resampling
```{r}
trainCtrl <- trainControl(method = "cv", number=3)
LDAmodelFit <- train(classe ~ ., method="lda", data=traindata, trControl=trainCtrl)
LDAmodelFit
```

To see how successful the LDA model is we run a confusion matrix.
```{r}
LDAConfusionmatrix <- confusionMatrix(testdata$classe,predict(LDAmodelFit,testdata))
LDAConfusionmatrix
```
The Accuracy of the model appears to be 64.1%.

Next we have elected to use a Generalised Boosted Regression (GBM) model with cross-validation involving 3 times resampling
```{r}
trainCtrl <- trainControl(method = "cv", number=3)
GBMmodelFit <- train(classe ~ ., method="gbm", data=traindata, trControl=trainCtrl, verbose = FALSE)
GBMmodelFit
```

To see how successful the GBM model is we run a confusion matrix.
```{r}
GBMConfusionmatrix <- confusionMatrix(testdata$classe,predict(GBMmodelFit,testdata))
GBMConfusionmatrix
```
The Accuracy of the model appears to be 94.9%.

Lets see what the optimal model parameters were.
```{r}
GBMmodelFit
```

These results are reflected in the following plot.
```{r}
plot(GBMmodelFit)
```

It appears that greater accuracy is achieved in the gbm model with more trees and greater depth of the analysis. An even better result may be had by changing either of these parameters.  

Finally we have elected to use a Random Forest (RF) model with cross-validation involving 3 times resampling
```{r}
trainCtrl <- trainControl(method = "cv", number=3)
RFmodelFit <- train(classe ~ ., method="rf", data=traindata, trControl=trainCtrl, importance=TRUE)
RFmodelFit
```

To see how successful the RF model is we run a confusion matrix.
```{r}
RFConfusionmatrix <- confusionMatrix(testdata$classe,predict(RFmodelFit,testdata))
RFConfusionmatrix
```

Lets see what the estimated overall error rate is:
```{r}
RFmodelFit$finalModel
```

It can be seen that the Out Of Bag estimate of error rate is 0.75%
The accuracy is: 99.2% which is reflected in the following plot.

```{r}
plot(RFmodelFit)
```

It appears that the RF model is extremely accurate with over 99% accuracy across each classe grouping. This was closely followed by the gbm boosting model at just under 95% accuracy.The LDA model followed way behind with only a 64% accuracy.

##In Conclusion

So what are the answers given by each of the models?

Linear Discriminant Analysis:
```{r}
LDAtest_answers <- predict(LDAmodelFit, testingSubset[,-41])
```

GBM:
```{r}
GBMtest_answers <- predict(GBMmodelFit, testingSubset[,-41])
```

Random Forest:
```{r}
RFtest_answers <- predict(RFmodelFit, testingSubset[,-41])
```

Let's look at the results of each test when the prediction models are applied:
```{r}
answers <- as.data.frame(matrix(seq(NA), nrow=3, ncol=21))

names(answers)<-c('Test Name',
                  'Test 1','Test 2','Test 3','Test 4','Test 5',
                  'Test 6','Test 7','Test 8','Test 9','Test 10',
                  'Test 11','Test 12','Test 13','Test 14','Test 15',
                  'Test 16','Test 17','Test 18','Test 19','Test 20')

answers[1,1] <- 'LDA'
answers[2,1] <- 'GMB'
answers[3,1] <- 'RF'

answers[1,2:21] <- as.factor(LDAtest_answers)
answers[2,2:21] <- as.factor(GBMtest_answers)
answers[3,2:21] <- as.factor(RFtest_answers)

answers
```

From these results it appears that the minor difference in accuracy between the RF and GMB models made no difference in the outcome when applied to the test set. The LDA model, however, produced vastly different results.

On the basis of accuracy alone, the present analysis has decided to choose the Random Forest model.